% Encoding: UTF-8

@InProceedings{SommerDavisChevalier2019,
  author    = {{Sommer}, P.~S. and {Davis}, B.~A.~S. and {Chevalier}, M.},
  title     = {{Github and Open Research Data; an example using the Eurasian Modern Pollen Database}},
  booktitle = {EGU General Assembly Conference Abstracts},
  year      = {2019},
  volume    = {21},
  series    = {EGU General Assembly Conference Abstracts},
  pages     = {5669},
  month     = apr,
  abstract  = {Established in 2011, the Eurasian Modern Pollen Database (EMPD) is a standardized, fully documented and quality-controlled dataset of over 8000 modern pollen samples which can be openly accessed, and to which scientists can also contribute and help maintain. The database has recently been upgraded to include an intuitive client-based JavaScript web-interface hosted on the version control system Github, allowing data and metadata to be accessed and viewed using a clickable map. We present how we manage the FAIR principles, such as well-documented access and handling of data and metadata using the free Github services for open source development, as well as other critical points for open research data, such as data accreditation and referencing.
Our community-based framework allows automated and transparent quality checks through continuous integration, fast and intuitive access to the data, as well as transparency for data contributors and users concerning changes and bugs in the EMPD. Furthermore, it allows a stable and long-lasting access to the web interface (and the data) without any funding requirements for servers or the risk of security holes.},
  url       = {https://meetingorganizer.copernicus.org/EGU2019/EGU2019-5669.pdf},
}

@InProceedings{SommerKaplan2016,
  author    = {{Sommer}, P. and {Kaplan}, J.},
  title     = {{Fundamental statistical relationships between monthly and daily meteorological variables: Temporal downscaling of weather based on a global observational dataset}},
  booktitle = {EGU General Assembly Conference Abstracts},
  year      = {2016},
  volume    = {18},
  series    = {EGU General Assembly Conference Abstracts},
  pages     = {EPSC2016-18183},
  month     = apr,
  note      = {Provided by the SAO/NASA Astrophysics Data System},
  eid       = {EPSC2016-18183},
  url       = {http://adsabs.harvard.edu/abs/2016EGUGA..1818183S},
  pdf_file  = {http://presentations.copernicus.org/EGU2016-18183_presentation.pdf},
  abstract  = {Accurate modelling of large-scale vegetation dynamics, hydrology, and other environmental processes requires meteorological forcing on daily timescales. While meteorological data with high temporal resolution is becoming increasingly available, simulations for the future or distant past are limited by lack of data and poor performance of climate models, e.g., in simulating daily precipitation. To overcome these limitations, we may temporally downscale monthly summary data to a daily time step using a weather generator. Parameterization of such statistical models has traditionally been based on a limited number of observations. Recent developments in the archiving, distribution, and analysis of "big data" datasets provide new opportunities for the parameterization of a temporal downscaling model that is applicable over a wide range of climates. Here we parameterize a WGEN-type weather generator using more than 50 million individual daily meteorological observations, from over 10'000 stations covering all continents, based on the Global Historical Climatology Network (GHCN) and Synoptic Cloud Reports (EECRA) databases. Using the resulting "universal" parameterization and driven by monthly summaries, we downscale mean temperature (minimum and maximum), cloud cover, and total precipitation, to daily estimates. We apply a hybrid gamma-generalized Pareto distribution to calculate daily precipitation amounts, which overcomes much of the inability of earlier weather generators to simulate high amounts of daily precipitation. Our globally parameterized weather generator has numerous applications, including vegetation and crop modelling for paleoenvironmental studies.},
}

@InProceedings{Sommer2016,
  author    = {{Sommer}, P.},
  title     = {{Psyplot: Visualizing rectangular and triangular Climate Model Data with Python}},
  booktitle = {EGU General Assembly Conference Abstracts},
  year      = {2016},
  volume    = {18},
  series    = {EGU General Assembly Conference Abstracts},
  pages     = {18185},
  month     = apr,
  note      = {Provided by the SAO/NASA Astrophysics Data System},
  url       = {http://adsabs.harvard.edu/abs/2016EGUGA..1818185S},
  pdf_file  = {http://presentations.copernicus.org/EGU2016-18185_presentation.pdf},
  abstract  = {The development and use of climate models often requires the visualization of geo-referenced data. Creating visualizations should be fast, attractive, flexible, easily applicable and easily reproducible. There is a wide range of software tools available for visualizing raster data, but they often are inaccessible to many users (e.g. because they are difficult to use in a script or have low flexibility). In order to facilitate easy visualization of geo-referenced data, we developed a new framework called "psyplot," which can aid earth system scientists with their daily work. It is purely written in the programming language Python and primarily built upon the python packages matplotlib, cartopy and xray. The package can visualize data stored on the hard disk (e.g. NetCDF, GeoTIFF, any other file format supported by the xray package), or directly from the memory or Climate Data Operators (CDOs). Furthermore, data can be visualized on a rectangular grid (following or not following the CF Conventions) and on a triangular grid (following the CF or UGRID Conventions). Psyplot visualizes 2D scalar and vector fields, enabling the user to easily manage and format multiple plots at the same time, and to export the plots into all common picture formats and movies covered by the matplotlib package. The package can currently be used in an interactive python session or in python scripts, and will soon be developed for use with a graphical user interface (GUI). Finally, the psyplot framework enables flexible configuration, allows easy integration into other scripts that uses matplotlib, and provides a flexible foundation for further development.},
}

@InProceedings{SommerKaplan2016a,
  author    = {{Sommer}, P. and {Kaplan}, J.},
  title     = {{Fundamental statistical relationships between monthly and daily meteorological variables: Temporal downscaling of weather based on a global observational dataset}},
  booktitle = {Workshop on Stochastic Weather Generators},
  year      = {2016},
  address   = {Vannes (France)},
  month     = {may},
  publisher = {University of Bretagne Sud},
  url       = {https://www.lebesgue.fr/content/sem2016-climate-program},
  abstract  = {Accurate  modelling  of  large-scale  vegetation  dynamics,  hydrology,  and  otherenvironmental  processes  requires  meteorological  forcing  on  daily  timescales.   Whilemeteorological data with high temporal resolution is becoming increasingly available,simulations for the future or distant past are limited by lack of data and poor perfor-mance of climate models,  e.g.,  in simulating daily precipitation.  To overcome theselimitations,  we  may  temporally  downscale  monthly  summary  data  to  a  daily  timestep using a weather generator.  Parameterization of such statistical models has tradi-tionally been based on a limited number of observations.  Recent developments in thearchiving, distribution, and analysis of big data datasets provide new opportunities forthe parameterization of a temporal downscaling model that is applicable over a widerange of climates.  Here we parameterize a WGEN-type weather generator using morethan 50 million individual daily meteorological observations, from over 10’000 stationscovering all continents, based on the Global Historical Climatology Network (GHCN)and  Synoptic  Cloud  Reports  (EECRA)  databases.   Using  the  resulting  “universal”parameterization and driven by monthly summaries, we downscale mean temperature(minimum  and  maximum),  cloud  cover,  and  total  precipitation,  to  daily  estimates.We apply a hybrid gamma-generalized Pareto distribution to calculate daily precipi-tation amounts, which overcomes much of the inability of earlier weather generatorsto simulate high amounts of daily precipitation.  Our globally parameterized weathergenerator has numerous applications, including vegetation and crop modelling for pa-leoenvironmental studies.},
}

@InProceedings{SommerKaplan2017c,
  author    = {{Sommer}, P. and {Kaplan}, J.},
  title     = {{Quantitative Modeling of Human-Environment Interactions in Preindustrial Time}},
  booktitle = {PAGES OSM 2017, Abstract Book},
  year      = {2017},
  pages     = {129-129},
  month     = apr,
  adsurl    = {http://pastglobalchanges.org/osm2017/downloads/osm-abstract-book-zaragoza-2017.pdf},
  eid       = {02126, 23},
}

@InProceedings{Sommer2018g,
  author    = {{Sommer}, P.~S.},
  title     = {{Psyplot: Interactive data analysis and visualization with Python}},
  booktitle = {EGU General Assembly Conference Abstracts},
  year      = {2018},
  volume    = {20},
  series    = {EGU General Assembly Conference Abstracts},
  pages     = {4701},
  month     = apr,
  note      = {Provided by the SAO/NASA Astrophysics Data System},
  url       = {http://adsabs.harvard.edu/abs/2018EGUGA..20.4701S},
  pdf_file  = {http://presentations.copernicus.org/EGU2018-4701_presentation.pdf},
  abstract  = {The development, usage and analysis of climate models often requires the visualization of the data. This visualization should ideally be nice looking, simple in application, fast, easy reproducible and flexible. There exist a wide range of software tools to visualize model data which however often lack in their ability of being (easy) scriptable, have low flexibility or simply are far too complex for a quick look into the data. Therefore, we developed the open-source visualization framework psyplot that aims to cover the visualization in the daily work of earth system scientists working with data of the climate system. It is build (mainly) upon the python packages matplotlib, cartopy and xarray and integrates the visualization process into data analysis. This data can either be stored in a NetCDF, GeoTIFF, or any other format that is handled by the xarray package. Due to its interactive nature however, it may also be used with data that is currently processed and not already stored on the hard disk. Visualizations of rastered data on the glob are supported for rectangular grids (following or not following the CF Conventions) or on a triangular grid (following the CF Conventions (like the earth system model ICON) or the unstructured grid conventions (UGRID)). Furthermore, the package visualizes scalar and vector fields, enables to easily manage and format multiple plots at the same time. Psyplot can either be used with only a few lines of code from the command line in an interactive python session, via python scripts or from through a graphical user interface (GUI). Finally, the framework developed in this package enables a very flexible configuration, an easy integration into other scripts using matplotlib.},
}

@InProceedings{SommerDavisChevalier2018,
  author    = {{Sommer}, P.~S. and {Davis}, B.~A.~S. and {Chevalier}, M.},
  title     = {{STRADITIZE: An open-source program for digitizing pollen diagrams and other types of stratigraphic data}},
  booktitle = {EGU General Assembly Conference Abstracts},
  year      = {2018},
  volume    = {20},
  series    = {EGU General Assembly Conference Abstracts},
  pages     = {4433},
  month     = apr,
  note      = {Provided by the SAO/NASA Astrophysics Data System},
  url       = {http://adsabs.harvard.edu/abs/2018EGUGA..20.4433S},
  pdf_file  = {http://presentations.copernicus.org/EGU2018-4433_presentation.pdf},
  abstract  = {In an age of digital data analysis, gaining access to data from the pre-digital era - or any data that is only available as a figure on a page - remains a problem and an under-utilized scientific resource. Whilst there are numerous programs available that allow the digitization of scientific data in a simple x-y graph format, we know of no semi-automated program that can deal with data plotted with multiple horizontal axes that share the same vertical axis, such as pollen diagrams and other stratigraphic figures that are common in the Earth sciences. STRADITIZE (Stratigraphic Diagram Digitizer) is a new open-source program that allows stratigraphic figures to be digitized in a single semi-automated operation. It is designed to detect multiple plots of variables analyzed along the same vertical axis, whether this is a sediment core or any similar depth/time series. The program is written in python and supports mixtures of many different diagram types, such as bar plots, line plots, as well as shaded, stacked, and filled area plots. The package provides an extensively documented graphical user interface for a point-and-click handling of the semi-automatic process, but can also be scripted or used from the command line. Other features of STRADITIZE include text recognition to interpret the names of the different plotted variables, the automatic and semi-automatic recognition of picture artifacts, as well an automatic measurement finder to exactly reproduce the data that has been used to create the diagram. Evaluation of the program has been undertaken comparing the digitization of published figures with the original digital data. This generally shows very good results, although this is inevitably reliant on the quality and resolution of the original figure.},
}

@InProceedings{SommerChevalierDavis2018,
  author    = {Philipp S. Sommer and Manuel Chevalier and Basil A. S. Davis},
  title     = {{STRADITIZE: An open-source program for digitizing pollen diagrams and other types of stratigraphic data}},
  booktitle = {AFQUA - The African Quaternary},
  year      = {2018},
  address   = {Nairobi (Kenya)},
  month     = {july},
  publisher = {AFQUA},
  abstract  = {Straditize (Stratigraphic Diagram Digitizer) is a new open-source program that allows stratigraphic diagrams to be digitized in a single semi-automated operation. It is specifically designed for figures that have multiple horizontal axes plotted against a shared vertical axis (e.g. depth/age), such as pollen diagrams.},
  url       = {https://afquacongress.wixsite.com/afqua2018},
}

@InProceedings{SommerDavisChevalierEtAl2019,
  author    = {Philipp S. Sommer and Basil A. S. Davis and Manuel Chevalier and Jian Ni and John Tipton},
  title     = {The HORNET project: applying 'big data' to reconstruct the climate of the Northern Hemisphere during the Holocene},
  booktitle = {20th Congress of the International Union for Quaternary Research (INQUA)},
  year      = {2019},
  publisher = {International Union for Quaternary Research},
  abstract  = {Pollen data remains one of the most widely geographically distributed, publicly accessible and most thoroughly documented sources of quantitative palaeoclimate data. It represents one of the primary terrestrial proxies in understanding the spatial pattern of past climate change at centennial to millennial timescales, and a great example of 'big data' in the palaeoclimate sciences. The HORNET project is based on the synthesis and analysis of thousands of fossil and modern pollen samples to create a spatially and seasonally explicit record of climate change covering the whole Northern Hemisphere over the last 12,000 years, using a common reconstruction and error accounting methodology. This type of study has been made possible only through long-term community led efforts to advance the availability of 'open big data', and represents a good example of what can now be achieved within this new paradigm.

Primary pollen data for the HORNET project was collected not only from open public databases such as Neotoma, Pangaea and the European Pollen Database, but also by encouraging individual scientists and research groups to share their data for the purposes of the project and these open databases, and through the use of specifically developed digitisation tools which can bring previously inaccessible data into this open digital world. The resulting project database includes over 3000 fossil pollen sites, as well as 16000 modern pollen samples for use in the pollen-climate calibration transfer-function. Building and managing such a large database has been a considerable challenge that has been met primarily through the application and development of open source software, which provide important cost and resource effective tools for the analysis of open data.

The HORNET database can be interfaced through a newly developed, simple, freely accessible, and intuitive clickable map based web interface. This interface, hosted on the version control system Github, has been used mainly for quality control, method development and sharing the results and source database. Additionally, it provides the opportunity for other applications such as the comparison with other reconstructions based on other proxies, which we have also included in the database. We present the challenges in building and sharing such a large open database within the typically limited resources and funding that most scientific projects operate.
Pollen data remains one of the most widely geographically distributed, publicly accessible and most thoroughly documented sources of quantitative palaeoclimate data. It represents one of the primary terrestrial proxies in understanding the spatial pattern of past climate change at centennial to millennial timescales, and a great example of 'big data' in the palaeoclimate sciences. The HORNET project is based on the synthesis and analysis of thousands of fossil and modern pollen samples to create a spatially and seasonally explicit record of climate change covering the whole Northern Hemisphere over the last 12,000 years, using a common reconstruction and error accounting methodology. This type of study has been made possible only through long-term community led efforts to advance the availability of 'open big data', and represents a good example of what can now be achieved within this new paradigm.

Primary pollen data for the HORNET project was collected not only from open public databases such as Neotoma, Pangaea and the European Pollen Database, but also by encouraging individual scientists and research groups to share their data for the purposes of the project and these open databases, and through the use of specifically developed digitisation tools which can bring previously inaccessible data into this open digital world. The resulting project database includes over 3000 fossil pollen sites, as well as 16000 modern pollen samples for use in the pollen-climate calibration transfer-function. Building and managing such a large database has been a considerable challenge that has been met primarily through the application and development of open source software, which provide important cost and resource effective tools for the analysis of open data.

The HORNET database can be interfaced through a newly developed, simple, freely accessible, and intuitive clickable map based web interface. This interface, hosted on the version control system Github, has been used mainly for quality control, method development and sharing the results and source database. Additionally, it provides the opportunity for other applications such as the comparison with other reconstructions based on other proxies, which we have also included in the database. We present the challenges in building and sharing such a large open database within the typically limited resources and funding that most scientific projects operate.
},
  url       = {https://app.oxfordabstracts.com/events/574/program-app/submission/94623},
}