% Introduction chapter

\chapter{Introduction}

\label{chp:intro}

\begin{refsection}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Motivation} \label{sec:motivation}

Climate science and in particular the study of past climates face an increasing need for the analysis, standardization and sharing of data. Scientists made huge efforts to explore climate archives throughout the world to investigate the evolution of the Earths climate\addref. In parallel, computational climate models grew in complexity and data ouput due to an increase of computational power and the availability of supercomputers\addref. This generates new challenges for big data analysis that can only solved by high quality and flexible software packages.

The Neotoma Database, a global international database for paleoenvironmental proxies \citep{WilliamsGrimmBloisEtAl2018} currently lists XXX datasets with in total XXX samples for past 12'000 years, the Holocene. Such data collections enable large-scale reconstructions of past climates that however face considerable challenges. They mainly arise from the heterogeneity of the data and the necessity of further quality control and standardization. One key problem is the accessibility of data. A lot of data is not available in standardized relational databases and either held private, or is stored in less standardized archives such as PANGAEA\addref[\url{https://pangaea.de/}], or is not available in a digital format at all. The latter often results in the need of digitizing the associated data from a published diagram, a tedious and imprecise task \citep{SommerRechChevalierEtAl2019}. Additionally handling such a big heterogeneous data resource and analyzing its contents is a key challenge and requires flexible visualization resources that efficiently allow the querying of spatial data with heterogeneous time and meta data information\addref[EMPD paper].

An additional challenge arises from the combination with numerical models that usually operate on a structured \citep{Edwards2010,TreutSomervilleCubaschEtAl2007} or unstructured \addref[ICON] grid with a fixed timestep. The development and analysis of such models requires visualization techniques that are interoperable with the specific data structure of the model \citep[e.g.][]{RewDavis1990, BrownFolkGoucherEtAl1993} while still being flexible enough for general purposes and computations \citep{Sommer2017, HoyerHamman2017}. Additionally it requires techniques to process observational data to make it comparable with climate models \citep{MauriDavisCollinsEtAl2015} \addref[POLNET-gridding paper] or to feed a model with the data using data assimilation of statistical models \citep{SommerKaplan2017b}.

In the following section \ref{sec:intro-paleo} I will lay down the interest in the study of paleo-climates, both from the observational and the modellers perspective. This is continued by a section \ref{sec:intro-software} which highlights the specific requirements and the historical development of software in paleo-science and concludes with section \ref{sec:intro-software-tools} that provides an overview on the contents of this thesis.

\section{Learning from the Past â€“ Why we study paleo-climates} \label{sec:intro-paleo}

Mankind is facing large infrastructural challenges during this century, such as the loss of biodiversity\addref, an exponentially growing world population \addref and an acceleration of growth and globalization of markets\addref[cite World bank report?]. They all interact with a global climate change that may lead to a new environment none of us ever experienced. Any future global planning has to account highly diverse responses that range from regional to continental scales. The complex (climate) system will enter a state that is significantly different from everything we had since the beginning of the satellite era in the 19th century, the beginning of global meteorological data acquisition\addref.

Our knowledge about this new climate is therefore mainly based on computational \glspl{esm}. They face the challenge of simulating a new climate based on our present knowledge of the interactions between the different compartments Ocean, Land and Atmosphere. Running such a model for the entire Earth with a reasonable resolution is therefore very cost-intensive and requires large computational resources. The validation of it becomes technically difficult considering the large amount of data output, and additionally conceptually difficult because of the aforementioned transition into a warmer world during the next century. We are entering a new state and it is questionable how well our models perform \citep{UldenOldenborgh2006, Karpechko2010, HargreavesAnnanOhgaitoEtAl2013}\addref[check these references! taken from Achilles PhD thesis, there might be better ones].

To evaluate their skill, we can only use our knowledge of the past climate from before the systematic measurement of temperature, precipitation, etc. These climates, also referred to as paleo-climates, provide the only opportunity to evaluate an \gls{esm} under conditions very different than today. paleo-climatic research has therefore been an integral part for climate sciences since the 80s \citep{COHMAPMembers1988, JoussaumeTaylor1995}, particularly in the \glsfirst{pmip} \citep{BraconnotOttoBliesnerHarrisonEtAl2007, BraconnotOttoBliesnerHarrisonEtAl2007a, BraconnotHarrisonKageyamaEtAl2012, KageyamaBraconnotHarrisonEtAl2016, Otto-BliesnerBraconnotHarrisonEtAl2017, JungclausBardBaroniEtAl2017}.

The current geological period is the Quaternary. It is characterized by glacial-interglacial cycles mainly driven by orbital changes \citep{HaysImbrieShackleton1976, ImbrieBergerBoyleEtAl1993}\addref[Check these] that cause a varying insolation on the planet.

The end of this period can be used for data-model comparisons due to the availability of paleo-climate archives. It started with the \gls{lig} about 127'000 years ago and was followed by the \gls{lgm} at 21'000 years ago. The warming of the atmosphere in the following interglacial  has been interrupted by a rapid cooling, called the Younger Dryas, between 12'900 and 11'700 years ago, which then let to the onset of the current epoch, the Holocene \citep{WalkerJohnsenRasmussenEtAl2009} \addref[check \cite{WalkerJohnsenRasmussenEtAl2009}].

\todo[inline, size=\normalsize]{Add some background on the Holocene. How did it change (global mean temperature estimate?), how was the insolation? CO$_2$ effects, impact of the ice sheets during the early holocene, changes in altitude, large-scale atmospheric circulation, human influences.}

This epoch is of particular interest because the continental setup is comparable to nowadays while still having a climate that is significantly different from present day \addref[PMIP paper]. Additionally we have a large set of proxies available to quantify the climate, independent from the model estimates, and for the entire globe \citep{WannerBeerButikoferEtAl2008} \addref[check \cite{WannerBeerButikoferEtAl2008}].


\subsection{Proxy Data from the Holocene}  \label{sec:intro-paleo-data}
Before 1850, there is almost no instrumental measurement of temperature. Instead we rely on archives such as lake sediments, glaciers, peat bogs, or speleothems that preserve climate proxies. The latter is a set of variables that are influenced by climate conditions and therefore allow an indirect measurement of climate parameters at ancient times, e.g. temperature, precipitation or sea-level. The most prominent proxies are isotopic compositions of $\delta^{18}$O in glacial ice cores\addref, marine sediments\addref, peat bogs\addref or speleothems\addref; bio-ecologic assemblages such as pollen\addref, chironomids \addref or diatoms \addref in lake sediments; foraminifera and alkenone in marine sediments\addref; and the widths of tree rings.

The most abundant climate proxy, that I will also focus on in the next chapters, are pollen assemblages. It is the  geographically most wide spread paleo-climate proxy \citep{BirksBirks1980} \addref[Don't know about \cite{BirksBirks1980}, took it from Manus review paper...] and has a long history in quantitative paleo-climatologic reconstructions \citep[e.g.][]{Nichols1967, Nichols1969, Bradley1985}.

The ability to serve as a proxy for the past arises from the chemically stable polymer sporopollenin, that allows it to be preserved over very long periods of time, in various environments such as lakes, wetlands or ocean sediments \addref[Manus review paper] \citep{FaegriKalandKrzywinski1989, Havinga1967}. Pollen are produced by seed-bearing plants (spermatophytes, \cite{Wodehouse1935} \addref[Don't know about \cite{Wodehouse1935}, took it from Manus review paper...]) and as such have a high spatial continuity and prevalence. Their compositions (closely related to the surrounding vegetation) is highly dependent on the climate and allows the reconstruction of the latter through an inverse modelling approach \addref[cite some MAT, WAPLS, Bayesian, etc. papers].

The usefulness for large-scale data-model intercomparisons additionally arises from the existence of regional databases for fossil pollen assemblages. The earliest examples are the \gls{epd} and \gls{napd}  that both started aroun 1990 and developed a similar structure in order to be compatible \citep{Grimm2008, FyfeBeaulieuBinneyEtAl2009} . This let to the development of other regional pollen databases, such as the \gls{lapd} \citep[LAPD, ][]{FlantuaHooghiemstraGrimmEtAl2015, MarchantAlmeidaBehlingEtAl2002} in 1994 or the \gls{apd} \citep{VincensLezineBuchetEtAl2007} in 1996, and others \citep[see][]{Grimm2008}. These attempts finally let to the development of the Neotoma database \citep{WilliamsGrimmBloisEtAl2018}, a global multiproxy database that incorporates many of the regional pollen databases.

The use of the above-mentioned proxies, particularly pollen, for paleo-climate reconstruction has a long academic tradition in geology \citep{Bradley1985} and provides the source of large-scale paleo-climatic reconstructions in number of different studies \citep{MauriDavisCollinsEtAl2015, DavisBrewerStevensonEtAl2003, MarsicekShumanBartleinEtAl2018, NeukomSteigerGomezNavarroEtAl2019, NeukomBarbozaErbEtAl2019}\addref[add more..., Climate12K]. They however have multiple uncertainties, that are often difficult to estimate and to consider. The main challenge for a data-model comparison are dating uncertainties, but often also about the influence of seasonality on the proxy (e.g. whether it represents summer, winter or annual temperature) and the quality of the record. Another challenge is the proper handling of uncertainties of the inverse modelling approach\addref[cite some MAT papers], spatial coverage of the proxy (see chapter \ref{chp:straditize}), and, considering pollen assemblages, the various naming schemes for pollen taxa that need to be considered for large-scale reconstructions\addref[that North-US/South-US discrepancy...].


\subsection{Model Simulations of the Holocene}  \label{sec:intro-paleo-model}
As mentioned in the earlier section \ref{sec:intro-paleo}, paleoclimate simulations of \glspl{esm} played in important role in previous intercomparisons. The Holocene analysis within past \gls{pmip} versions focused mainly on the mid-Holocene around 6000 years before present, a time period with a different latitudinal and seasonal distribution of incoming solar radiation (insolation) but greenhouse gas concentrations similar to the preindustrial period \citep{Otto-BliesnerBraconnotHarrisonEtAl2017}. The main findings from previous intercomparisons are an underestimation of polar amplification in \gls{pmip}2 and \gls{pmip}3 models due to sea ice and vegetational feedbacks, and an underestimation of the north-south temperature gradient over Europe \citep{ClimateChange2014a, MassonDelmotteKageyamaBraconnotEtAl2006, ZhangSundqvistMobergEtAl2010, BrewerGuiotTorre2007, DavisBrewer2009}.

The focus on only a short time slice (mainly due to the high computational costs for running an \gls{esm} over a large simulation period) however has several complications. Climate changes, i.e. the shift into a different climatic state, cannot be simulated and high dating uncertainties hinder a credible comparison of models and proxies\addref. Therefore multiple recent studies published and proposed increasing efforts for transient model simulations, i.e. simulations that cover multiple millenia during the last deglaciation \citep{IvanovicGregoireKageyamaEtAl2016} and the Holocene \citep{Otto-BliesnerBraconnotHarrisonEtAl2017}. Several studies used \glspl{emic} for transient simulations that cover parts of the last 12'000 years \citep[e.g.][]{RocheRenssenPaillardEtAl2011, MenvielTimmermannTimmEtAl2011, GregoireValdesPayne2015} and a few used a global coupled \gls{esm} \citep{VarmaPrangeMerkelEtAl2012, OttoBliesnerRussellClarkEtAl2014}. As stated by \cite{WeitzelWagnerSjolteEtAl2019}, those model results can clarify the role of internal climate variability for Holocene temperature trends and large-scale patterns.

Despite the computational costs for running these models, technical challenges arise from the size of the data that easily exceeds the size of multiple Gigabyte per climatic variable with a monthly resolution. It requires software that is able to deal with data too large to fit into memory (see section \ref{sec:intro-software-model} and chapter \ref{chp:psyplot}) and automated techniques to identify patterns in the data.


%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Software for Paleoclimatology} \label{sec:intro-software}

The usage of software is crucial for the quantitative reconstruction of Earth's Climate. Paleoclimate research is facing an information overload problem and requires innovative methodologies in the realm of visual analytics, the interplay between automated analysis techniques and interactive visualization \citep{KeimAndrienkoFeketeEtAl2008, Nocke2014}. As such, a visual representation of the paleoclimate reconstruction has been essential for both, proxies \citep{Nichols1967, Bradley1985, Grimm1988} and models \citep{Phillips1956, RautenhausBoettingerSiemenEtAl2018, NockeSterzelBoettingerEtAl2008, Nocke2014, BoettingerRoeber2019}, although the visualization methods significantly differ due to the differences in data size and data heterogeneity.

The second important aspect for software and paleoclimate is the distribution of data to make it accessible to other researchers, the community and policy makers\addref[cite some open-data publications], which is commonly established through online accessible data archives and recently also through map-based web interfaces \citep{WilliamsGrimmBloisEtAl2018, BollietBrockmannMassonDelmotteEtAl2016}.

The following sections provide an overview on the different techniques used by modelers and palynologists to visualize and distribute their data and concludes with an introduction into Open-Source Software Development, which forms the basis of the software solutions that are presented later in this thesis (chapters \ref{chp:psyplot}, \ref{chp:straditize}, and \ref{chp:empd}, and appendix \ref{chp:software}).

\subsection{Sofware for Proxy Data Analysis, Visualization and Distribution} \label{sec:intro-software-data}

Due to the nature of stratigraphic data, proxies, especially pollen assemblages, are often treated as a collection of multiple time-series (one-dimensional arrays). The size of one dataset is generally small (in the range of kB) and can be treated as plain text files. Traditionally, numerical and statistical analysis are separated from the visualization.

In palynology, standard analytical tools are Microsoft Excel\footnote{\url{https://products.office.com/en/excel}} and the R software for statistical computing \citep{RCT2019}. The latter also involves multiple packages for paleoclimatic reconstruction, such as \texttt{rioja} \citep{Juggins2017} and \texttt{analogue} \citep{SimpsonOksanen2019, Simpson2007} or bayesian methods \citep{NolanTiptonBoothEtAl2019, Tipton2017}\addref[add more?]. Alternatively, desktop applications exist, such as Polygon\footnote{\url{http://polsystems.rits-paleo.com}} by \cite{NakagawaTarasovNishidaEtAl2002} or the CREST software by \cite{ChevalierCheddadiChase2014, Chevalier2019}.

It is a long-standing tradition to visualize stratigraphic data, and especially pollen data, in form of a stratigraphic (pollen) diagram \citep{Bradley1985, Grimm1988}. Especially during the 19th century, when it was not yet common to distribute data alongside a peer-reviewed publication, pollen diagrams where the only possibility to publish the entire dataset (see also chapter \ref{chp:straditize}). The generation of these diagrams is usually based on desktop applications such as C2 \citep{Juggins2007}, Tilia\footnote{\url{https://www.tiliait.com/}} \citep{Grimm1988, Grimm1991}. A more recent implementation into the psyplot framework \citep[chapter \ref{chp:psyplot}]{Sommer2017} is also provided with the psy-strat plugin\footnote{\url{https://psy-strat.readthedocs.io}} \citep{Sommer2019}.

Raw pollen data is at present made available through web archives, such as PANGAEA\footnote{\url{https://pangaea.de/}} or the \gls{ncdc} by the \glsfirst{noaa}\footnote{\url{https://www.ncdc.noaa.gov/data-access/paleoclimatology-data}} where researchers can create a DOI for their raw data. Collections of data, such as regional pollen databases or project specific collections \citep[e.g.][]{WhitmoreGajewskiSawadaEtAl2005, DavisZanonCollinsEtAl2013} are usually published in one of the above-mentioned archives or associated with a publication. A different approach has been developed by \cite{BollietBrockmannMassonDelmotteEtAl2016} to develop a small web application as an interface into the data collection, the \textit{ClimateProxiesFinder} \citep[chapter \ref{chp:empd}]{Brockmann2016}.

Outstanding compared to the previous data interfaces is the new infrastructure for the Neotoma database \citep{WilliamsGrimmBloisEtAl2018}. It consists of the map-based web interface, the Neotoma Explorer\footnote{\url{https://apps.neotomadb.org/Explorer}}, a RESTful api\footnote{\url{https://api.neotomadb.org}} that allows an interaction with other web services, the neotoma R package \citep{GoringDawsonSimpsonEtAl2015} and an interface into the Tilia software for stratigraphic and map-based visualizations \citep{WilliamsGrimmBloisEtAl2018}. This rich functionality is, however, bound to the structure of Neotoma and as such, different from the Javascript-based approach developed in chapter \ref{chp:empd} cannot easily be transferred to other projects.


\subsection{The Development of Computational Climate Model Analysis} \label{sec:intro-software-model}

\begin{figure}
	\missingfigure{Visualize multiple grids on the same map, e.g. by using the grid specifications from \cite{TreutSomervilleCubaschEtAl2007}.}
	\caption{A selection of grid sizes and formats since the first IPCC report}
	\label{fig: grid-sizes}
\end{figure}

Software and computational numerics play a crucial row for our understanding of climate since the first \glspl{gcm} by \cite{Phillips1956} after world war II \citep{Edwards2010, Lewis1998}. The first simulations and analysis of \glspl{gcm} where limited by the available computational facilities, the model by \cite{Phillips1956} for example operated on a $17 \times 16$ grid simulating a surface with the size of roughly one tenth of the Earth. The possibilities of climate modeling increased rapidly, mainly due to a drastic increase in computational capacity and the availability of supercomputers. This let to an increase in speed by a factor of roughly one million between 1970 and 2007, permitting an increase in model complexity, length of the simulations, and spatial resolution \citep{TreutSomervilleCubaschEtAl2007}. In the past decade, unstructured grids raised more and more attention \citep{ZaenglReinertRipodasEtAl2014, SkamarockKlempDudaEtAl2012} especially with the focus on \textit{seamless prediction} \citep{Hoskins2012, BauerThorpeBrunet2015} that allows a refined grid resolution in selected regions of the earth \citep{RautenhausBoettingerSiemenEtAl2018}.

The varying grids, multi-dimensionality and volume of the data requires visual analytic methodologies much different from what is used with proxy data (section \ref{sec:intro-software-data}) and much more diverse. In general, scientists tend to disentangle the numeric post-processing of climate model output (such as computing aggregated statistics in time or space dimension) and then visualize these aggregated results \citep{BoettingerRoeber2019, SchulzNockeHeitzlerEtAl2013}. Common post-processing software are, for example \glspl{cdo} \citep{Schulzweida2019}, \glspl{nco} \citep{Zender2008, ZenderMangalam2007, Zender2016} and R \citep{RCT2019}, or more recently also python packages such as \texttt{xarray} \citep{HoyerHamman2017}. The choice of method thereby depends on the scientiests preference but also on the size of the data. Especially the analysis of transient model runs (section \ref{sec:intro-paleo-model}) requires software that is able to deal with data that is too big to fit into memory and requires parallel computation. Such an analysis can be done pursued with \glspl{cdo} and \texttt{xarray} through it's interface with the parallel computing package dask \citep{DDT2016, Rocklin2015} (see also chapter \ref{chp:gridding}).

\cite{RautenhausBoettingerSiemenEtAl2018} provide a detailed overview about how such a high amount of data is visualized. Methods range from 2D projection on a map to 3D interactive visualizations, depending on the background and knowledge of the researcher \citep{Nocke2014}. \cite{NockeSterzelBoettingerEtAl2008} recognize preference for script-based solutions such as Python, R or domain-specific languages such as NCL \citep{BrownBrownriggHaleyEtAl2012} that is still persistent today \citep{RautenhausBoettingerSiemenEtAl2018}. \cite{Nocke2014}, \cite{SchulzNockeHeitzlerEtAl2013} and \cite{RautenhausBoettingerSiemenEtAl2018} attribute this to the importance of comparability and reproducibility for research, and the usability in peer-reviewed publications. Therefore, 3D visualization, e.g. with ParaView \citep{Ayachit2015}, VAPOR \citep{ClyneMininniNortonEtAl2007} or Avizo\addref, are mainly used by visualization experts rather than scientists \citep{NockeSterzelBoettingerEtAl2008}\todo{Look into \cite{DasguptaPocoBertiniEtAl2016}}. However, \cite{Nocke2014} note that young researchers are more open for 3D visualization and especially with the newly emerging unstructured grids, they become more prominent. Besides psyplot \citep[chapter \ref{chp:psyplot}]{Sommer2017} and UV-CDAT\addref, there exists to my knowledge no scripted method that easily visualizes climate model data on unstructured grids, without the need for interpolation to a rectilinear grid. These have however been implemented for ParaView \citep{RoeberAdamidisBehrens2015} and Vapor \citep{JubairAlimRoeberEtAl2015} and new interfaces into VTK have also been developped by \cite{SullivanTrainorGuitton2019}.


\subsection{Methods and Workflows in Open-Source Software Development} \label{sec:intro-software-tools}

The importance and necessity of software for visualization and data analysis led to the development of the software packages presented in this thesis. Most of them are written in the programming language Python \citep{PerezGrangerHunter2011}, on the one hand due to my personal preference, but mainly due to the recent developments in out-of-core computing with the establishment of xarray and dask \citep{HoyerHamman2017, DDT2016, Rocklin2015}. Another important reason, especially for psyplot (chapter \ref{chp:psyplot}) and straditize (chapter \ref{chp:straditize}) was the availability of a highly flexible and stable package for graphical user interfaces, PyQt\addref, and the comparably simple possibility to implement an in-process python console into the PyQ5 application \addref[jupyter qtconsole] that allows to handle the software functionalities both, from the command line and from the GUI.

Modern \gls{foss} development is not only about making the source code available, but rather about providing a sustainable and maintainable package that allow continuous and transparent development under the aspect of rapidly evolving environment. In the following sections, I will introduce the most important \gls{foss} development concepts \citep[e.g.][]{StoddenMiguez2014, Shaw2018} and the necessary vocabulary. These concepts are used by many of the well-established software packages, such as matplotlib \citep{Hunter2007}, numpy \citep{Oliphant2006}, and scipy \citep{JonesOliphantPetersonEtAl2001}.

\subsubsection{Version Control} \label{sec:intro-software-github}
Version control systems record changes to a file and enables the user to roll-back to previous versions of it. The usage of a such a system is inevitable for sustainable \gls{foss} packages. It enables contributions by other \gls{foss} developers and the usage through external packages.

The packages I present in the following chapters are hosted on Github\footnote{The packages are available at \url{https://github.com/Chilipp}. Other potential platforms for version control are sourceforge (\url{https://sourceforge.net}) and Bitbucket (\url{https://bitbucket.org})}, a freely available web platform for hosting projects that are managed with git \citep{ChaconStraubPGC2019}.

Version control with git has a specific terminology. Central aspects are \textit{repositories} (project folders), \textit{commits} (change of the project files), \textit{issues} (bug reports), \textit{branches} and \textit{forks} (copies of the (main) project), and \textit{pull requests} (contributions to a project). The following list explains this vocabulary in a bit more detail, a more complete list is provided by in \cite{Github2019}.

\begin{description}
	\item[Repositories] are the most basic elements of git and Github. It can be compared to a folder that contains all the necessary files associated with a project (e.g. the source code and documentation of a software package). It also contains all the different versions (revisions) of the project files.
	\item[Commits] or revisions track the changes in the repository. Each commit is a change to a specific file (or a set of files) that is associated with a unique ID and a message of the author about to describe the changes.
	\item[Issues] are suggested improvements, bug reports or any other question to the repository. Every issue has a associated discussion page the topic can be discussed between repository owners and the users.
	\item[Branches] are parallel versions of a repository. Often one incorporates new developments into a separate branch that does not affect the main version of the repository (the \textit{master} branch) and merge the two versions when the new developments are fully implemented.
	\item[Forks] are copies of repositories. When someone wants to contribute to a software package (repository) that does not belong to him, he can \textit{fork} (copy) it, implement it's changes, and then create a \textit{pull request} to contribute to the official version. Different from a branch, that is a (modified) copy of another branch, forks are copies of the entire repository, i.e. all existing branches.
	\item[Pull request] are the proposed changes to a repository. One can create a fork of the repository from someone else, implement changes in this fork and then create a pull request to merge it into the original repository. Every pull request has an associated discussion page that allows the repository owner to moderate and discuss the suggested changes.
	\item[Webhooks] are general methods for web development. Github can trigger a hook to inform a different web service (such as a \gls{ci} (section \ref{sec:intro-software-ci})) that a repository has changed or that someone contributed in a discussion. In chapter \ref{chp:empd} we use Github webhooks for a automated administration of a repository.
\end{description}


\subsubsection{Automated Tests, Test Coverage and Continuous Integration} \label{sec:intro-software-ci}
The most important aspect for \gls{foss} development, especially considering the rapid evolution of this area, is the existence of automated tests. One distinguishes unit tests (test of one single routine) and integration tests (tests of one or more routines within the framework) \citep{Shaw2018}. The boundary between the two tests is rather vague and what is more appropriate highly depends on the structure of the software that is supposed to be tested. For complex frameworks (such as psyplot or straditize), integration tests are needed to ensure the operability within the framework. Other more simple software packages, (such as docrep or model-organization, see appendix \ref{sec:software-others}) go well with unit tests only.

Another good standard for such an test suite is to use an automated test discovery tool (e.g. the Python unittest package \citep{PSF2019} or pytest \citep{KrekelOliveiraPfannschmidtEtAl2004}) that also reports the test coverage (i.e. the fraction of the code that is tested by the test suite). These functionalities are then implemented on a \gls{ci} service, such as Travis CI\footnote{\url{https://travis-ci.org/}}, Appveyor\footnote{\url{https://appveyor.com}} or CircleCi\footnote{\url{https://circleci.com/}} that are integrated into the Github repository (section \ref{sec:intro-software-github}). Every commit to the Github repository or a new pull requests then triggers the tests. This transparently allows to ensure the operability of the software, and the test coverage report ensures that the newly implemented functionality is properly tested. A software development concept that is build entirely on that is the test-driven development. Within this framework, new features are implemented by starting with the test that should be fulfilled by the new feature and then improving the software until this test pass \citep{Beck2002}.


\subsubsection{Automated Documentation} \label{sec:intro-software-docs}
Documentation is the key aspect of a sustainable software and much of the geo-scientific code has a lack of proper documentation (based on personal experience). For the software in this thesis, for different levels of the documentation play an important role:

\begin{description}
	\item[The \gls{api} documentation] is meant to document the major parts of the software code that is subject to be used by external scripts or packages. It is usually implemented in the code and documents the essential subroutines and methods of the software.
	\item[The \gls{gui} documentation] provides help for the most high-level functionality for the software. The \gls{gui} is a user interface into the software through graphical elements (such as buttons, checkboxes, etc.). Unlike the \gls{api} documentation, it should not require knowledge about programming.
	\item[The contributing and/or developers guide] is targeting other software developers that might want to contribute to the software package. This document states how other software developers should contribute to the software and introduces the central structural aspects and frameworks of the software.
	\item[The manual] (or also commonly referred to as \textit{the} documentation) is the document that contains all necessary information for the software, such as installation instructions, tutorials, examples, etc.. It often includes some (or multiple) of the above parts.
\end{description}

The documentations for the software in this thesis have been automatically generated with Sphinx, a Python tool to generate documentations in various different formats (such as HTML, PDF, etc.)  \citep{PerezGrangerHunter2011, Hasecke2019}. It is also implemented as a webhook into the Github repository (see section \ref{sec:intro-software-github}) to automatically generate an up-to-date documentation of the software for each commit to the Github repository. This provides an additional automated test for the software, and especially it's high-level-interface, in addition to the automated test suite described above (section \ref{sec:intro-software-ci}). Most of the manuals are hosted and build online with the free services offered by \href{https://readthedocs.org/}{readthedocs.org}.


\subsubsection{Distribution through package managers and virtual environments} \label{sec:intro-software-conda}

\gls{foss} software is meant to be extensible and to build upon other \gls{foss} packages. This requires an accurate and transparent handling of it's dependencies and requirements which is usually provided through the so-called packaging of the software \citep[e.g.][]{Torborg2016}. There exists a variety of package managers and the choice most often depends on the framework of the software.

The software in this thesis is mainly distributed via two systems. The first one is python's own package manager \textit{pip} which is based on the packages uploaded to \href{https://pypi.org/}{pypi.org}. The second one which got increasing importance during the recent past is the open-source Anaconda Distribution\footnote{\url{https://www.anaconda.com}}. Both work on multiple operating systems (Windows, Linux and Mac OS), but the Anaconda Distribution contains also non-python packages (e.g. written in C or C++) for which the Python packages rely on, and it contains a rich suite of r-packages.

One step further, compared to package managers, are the distribution of virtual environments. These systems do not only provide the software, but also a full operating system and the installed dependencies. A popular platform (used also for the \gls{empd} database) is provided through so-called Docker containers\footnote{\url{https://www.docker.com}}. Compared to package managers, this system has the advantage of simplifying the installation procedure for the user because he only has to download the corresponding docker image. The docker image itself then runs independent of the local file system in a separate isolated mode.


%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Challenges tackled in this thesis} \label{sec:intro-thesis-overview}

I present several new tools in this thesis that tackle the aspects described in the previous sections. It is divided into two parts: Part \ref{part:software} are the software chapters \ref{chp:psyplot}, \ref{chp:straditize} and \ref{chp:empd} that introduce new packages developed for paleoclimate analysis. Part \ref{part:numerics} consists of the analysis chapters \ref{chp:gwgen} and \rewrite{\ref{chp:gridding} that address two use-cases tackling the combination of observations and models} for an informed paleoclimate understanding.

The first part starts with the visualization framework psyplot in chapter \ref{chp:psyplot}, a suite of python packages that are designed for interactive visual analysis of data both from a \gls{gui} and the command line. The scope of this software is not limited to paleoclimate analysis and serves a more general purpose. As such, it serves as a base infrastructure for many of the topics described in the other chapters.

Straditize, described in the next chapter \ref{chp:straditize}, addresses the problem of gathering paleo-climate information that has been collected during the pre-digital area. This software is a semi-automated digitization package for stratigraphic diagrams, particularly pollen diagrams. Straditize is built on top of psyplot and its \gls{gui} and as such provides a rich interactive documentation and visualization methods of the software tools.

Chapter \ref{chp:empd} covers the last aspect of software usage for paleoclimate data: data distribution. In this chapter I describe new infrastructural tools for the sustainable management of a community-driven pollen database, the \glsfirst{empd}. They consist of a flexible and lightweight map-based web interface, the EMPD-viewer, into the data and a webserver for an automated administration of the database. Within this section, I also present another use case for the EMPD-viewer that is adapted to a large northern-hemispheric database of fossil and modern pollen records.

The second part starts with the weather generator GWGEN in chapter \ref{chp:gwgen}, a statistical model that uses modern relationships in observational data to inform large-scale paleo climate models with temporally downscaled temperature, precipitation, cloud cover and wind speed records.

\rewrite{Finally, in chapter \ref{chp:gridding} I investigate the question to what extent large-scale atmospheric circulation features can be estimated from proxy data. In this analysis I analyze the long-term stability of spatial correlation patterns between surface temperature and northern hemispheric teleconnections based on three \glspl{esm}.}

This thesis finishes with the conclusions in chapter \ref{chp:conclusions} which summarizes the new tools and findings and provides an outlook for the further development of the methods presented. In the \hyperlink{appendix}{Appendix} I present another work that has developed in a cooperation which is also based on the infrastructural tools from psyplot and GWGEN (appendix \ref{chp:iucm}), and I provide a list of the publications during my thesis (appendix \ref{chp:publications}) and an overview about all the software packages that have been developed (appendix \ref{chp:software}).

\printbibliography[heading=subbibintoc]

\end{refsection}
